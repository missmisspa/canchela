{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuLhUQDjZxhZ",
        "outputId": "b27c8cfd-e37b-45cb-cfa4-2c4a4486fad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped RDD: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
            "Filtered RDD: [12, 14, 16, 18, 20]\n",
            "FlatMapped RDD: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
            "Distinct RDD: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
            "Key-Value RDD: [(1, 1), (0, 2), (1, 3), (0, 4), (1, 5), (0, 6), (1, 7), (0, 8), (1, 9), (0, 10)]\n",
            "Reduced RDD: [(1, 25), (0, 30)]\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"Big Data Analysis Lab #1\")\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "mapped_rdd = rdd.map(lambda x: x * 2) #each element are multiplied by two\n",
        "\n",
        "filtered_rdd = mapped_rdd.filter(lambda x: x > 10) #filter out elements greater than 10\n",
        "\n",
        "flat_mapped_rdd = filtered_rdd.flatMap(lambda x: (x, x + 1)) #the filtered elements generate a tuple (x, x+1) and the flatMap flattens this list of tuples into a single list\n",
        "\n",
        "distinct_rdd = flat_mapped_rdd.distinct() #removes all the duplicates in flated_mapped_rdd. since all of the elements are unique, it remains unchanged\n",
        "\n",
        "key_value_rdd = rdd.map(lambda x: (x % 2, x))#each elements are turned into key value pair (zero for even, one for odd)\n",
        "reduced_rdd = key_value_rdd.reduceByKey(lambda a, b: a + b) #aggregates the values for each unique key by summing them up\n",
        "\n",
        "print(\"Mapped RDD:\", mapped_rdd.collect())\n",
        "print(\"Filtered RDD:\", filtered_rdd.collect())\n",
        "print(\"FlatMapped RDD:\", flat_mapped_rdd.collect())\n",
        "print(\"Distinct RDD:\", distinct_rdd.collect())\n",
        "print(\"Key-Value RDD:\", key_value_rdd.collect())\n",
        "print(\"Reduced RDD:\", reduced_rdd.collect())\n",
        "\n",
        "sc.stop()\n"
      ]
    }
  ]
}