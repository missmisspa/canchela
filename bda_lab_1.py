# -*- coding: utf-8 -*-
"""BDA LAB#1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JukBNq68gv2AoIB3UqJKhcOZqE-itgYa
"""

from pyspark import SparkContext

sc = SparkContext("local", "Big Data Analysis Lab #1")

data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

rdd = sc.parallelize(data)

mapped_rdd = rdd.map(lambda x: x * 2) #each element are multiplied by two

filtered_rdd = mapped_rdd.filter(lambda x: x > 10) #filter out elements greater than 10

flat_mapped_rdd = filtered_rdd.flatMap(lambda x: (x, x + 1)) #the filtered elements generate a tuple (x, x+1) and the flatMap flattens this list of tuples into a single list

distinct_rdd = flat_mapped_rdd.distinct() #removes all the duplicates in flated_mapped_rdd. since all of the elements are unique, it remains unchanged

key_value_rdd = rdd.map(lambda x: (x % 2, x))#each elements are turned into key value pair (zero for even, one for odd)
reduced_rdd = key_value_rdd.reduceByKey(lambda a, b: a + b) #aggregates the values for each unique key by summing them up

print("Mapped RDD:", mapped_rdd.collect())
print("Filtered RDD:", filtered_rdd.collect())
print("FlatMapped RDD:", flat_mapped_rdd.collect())
print("Distinct RDD:", distinct_rdd.collect())
print("Key-Value RDD:", key_value_rdd.collect())
print("Reduced RDD:", reduced_rdd.collect())

sc.stop()